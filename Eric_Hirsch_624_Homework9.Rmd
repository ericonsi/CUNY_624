---
title: "Eric_Hirsch_624_Homework_9"

output:
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r}

library(tidyverse)
library(EHData)
library(gridExtra)
library(caret)
library(AppliedPredictiveModeling)
library(randomForest)
library(Cubist)

```

8.1, 8.2, 8.3, 8.7

#### 8.1. Recreate the simulated data from Exercise 7.2:  

```{r}

library(mlbench)  
set.seed(200)  
simulated <- mlbench.friedman1(200, sd = 1) 
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated) 
colnames(simulated)[ncol(simulated)] <- "y"  
```

__(a) Fit a random forest model to all of the predictors, then estimate the  variable importance scores:__ 

```{r}
library(randomForest)
model1 <- randomForest(y ~., data = simulated, importance = TRUE, ntree = 1000)
rfImp1 <- varImp(model1, scale = FALSE) 
rfImp1
```


__Did the random forest model significantly use the uninformative predictors (V6 â€“ V10)?__

No, the predictors V6 through V10 have varimps that are either very low or negative.  

__(b) Now add an additional predictor that is highly correlated with one of the  informative predictors. For example:__

```{r}

simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1
cor(simulated$duplicate1, simulated$V1)  

```

```{r}

library(randomForest)
model2 <- randomForest(y ~., data = simulated, importance = TRUE, ntree = 1000)
rfImp1 <- varImp(model2, scale = FALSE) 
rfImp1

```


__Fit another random forest model to these data. Did the importance score  for V1 change? What happens when you add another predictor that is  also highly correlated with V1?__ 

The varimp is split between the two predictors. V1 drops and Duplicate1 picks up the slack.

__(c) Use the cforest function in the party package to fit a random forest model  using conditional inference trees. The party package function varimp can  calculate predictor importance. The conditional argument of that function toggles between the traditional importance measure and the modified  version described in Strobl et al. (2007). Do these importances show the  same pattern as the traditional random forest model?__

```{r}
library(party)
set.seed(042760)
cm <- cforest(y ~ ., data = simulated)
```

```{r}

varimp(cm)
varimp(cm, conditional = TRUE)

```

The pattern is roughly the same, although the distribution of importance among the significant predictors is somewhat changed.

__(d) Repeat this process with different tree models, such as boosted trees and  Cubist. Does the same pattern occur?__

__1) Boosted tree__
```{r}

library(gbm)
boosted <- gbm(y ~., data = simulated)
summary.gbm(boosted)


```


__2) Cubist__

```{r}

library(Cubist)

simulated1 <- simulated %>%
  dplyr::select(-y)

target <- simulated[,"y"]

c1 <- cubist(x = simulated1, y=target, committees = 100)
varImp(c1)

```


The two models show a similar pattern except that v6 and v8 have more importance in the cubist model.

#### 8.2. Use a simulation to show tree bias with different granularities.

```{r}
v1 <- sample(0:1000, 500, replace = T)
v2 <- sample(0:1000, 500, replace = T)
v3 <- sample(0:1000, 500, replace = T)
target = v1 + 2*v2 + 5*v3 + rnorm(500, sd=1000)

dfHigh <- data.frame(v1, v2, v3, target, stringsAsFactors = FALSE)
dfLow <- dfHigh

aggregate <-function(v) {
  
  v <- v/100
  v <- round(v,0)
  v <- v*100
  return (v)
  
}

dfLow$v1 <- aggregate(dfLow$v1)
dfLow$v2 <- aggregate(dfLow$v2)
dfLow$v3 <- aggregate(dfLow$v3)

```



```{r}

model3 <- randomForest(target ~., data = dfHigh, importance = TRUE)
rfImp3 <- varImp(model3, scale = FALSE) 
model3
rfImp3

model4 <- randomForest(target ~., data = dfLow, importance = TRUE)
rfImp4 <- varImp(model4, scale = FALSE) 
model4
rfImp4
```

I used two dataframes of simulated data, each with 3 variables and the same y.  The only difference was the low granularity dataframe rounded to the nearest 100. The high granularity model explained slightly more of the variance than the low granularity, and tended to spread out the explanatory power more evenly among the predictors.

#### 8.3. In stochastic gradient boosting the bagging fraction and learning rate  will govern the construction of the trees as they are guided by the gradient. Although the optimal values of these parameters should be obtained  through the tuning process, it is helpful to understand how the magnitudes  of these parameters affect magnitudes of variable importance. Figure 8.24  provides the variable importance plots for boosting using two extreme values  for the bagging fraction (0.1 and 0.9) and the learning rate (0.1 and 0.9) for  the solubility data. The left-hand plot has both parameters set to 0.1, and  the right-hand plot has both set to 0.9:  

__(a) Why does the model on the right focus its importance on just the first few  of predictors, whereas the model on the left spreads importance across  more predictors?__

When both parameters are set high the model will have a tendency to overfit.  It is interesting to note that an overfit model in a boosting algorithm may result in a model with fewer predictors at higher levels of importance.  In some models, as we begin to overfit we may pick up small effects in the training data that add features to our model.  Presumably because boosting is iterative, overfitting drops out features as high importance features take over,

__(b) Which model do you think would be more predictive of other samples?__

I would presume the left hand model.  On the other hand, a model with a low bagging fraction might underfit the data.

__(c) How would increasing interaction depth affect the slope of predictor importance for either model in Fig. 8.24?__

Interaction depth specifies the maximum depth for each tree (starting from a single node).  Increasing the depth also works in the direction of overfitting.  Therefore, the tendency will be toward fewer predictors with higher importance.

#### 8.7. Refer to Exercises 6.3 and 7.5 which describe a chemical manufacturing  process. Use the same data imputation, data splitting, and pre-processing  steps as before and train several tree-based models:  

```{r}
data(ChemicalManufacturingProcess)

dfChem <- ChemicalManufacturingProcess
dfChem2 <- EHPrepare_MissingValues_Imputation(dfChem, impute = "median")
dfChem3 <- EHPrepare_ScaleAllButTarget(dfChem2, "Yield")

set.seed(042760)

i <- createDataPartition(dfChem3[,1], p = .8, list=FALSE)

dfTrain <- dfChem3[i, ]
dfTest <- dfChem3[-i, ]

dfTrain2 <- dfTrain[,-58]
dfTest2 <- dfTest[,-58]
yTrain <- dfTrain[,58]
yTest <- dfTest[,58]

```

__(a) Which tree-based regression model gives the optimal resampling and test  set performance?__

__1) Random Forest__

```{r}
set.seed(042760)
model1 <- randomForest(Yield ~., data = dfTrain, importance = TRUE, ntree = 1000)

rfPred <- predict(model1, newdata = dfTest2)
postResample(pred =rfPred, obs = yTest) 
```

__2) Boosted__
```{r}
set.seed(042760)
library(gbm)
boosted <- gbm(Yield ~., data = dfTrain)
boosted

boostPred <- predict(boosted, newdata = dfTest2)
postResample(pred =boostPred, obs = yTest) 
```

__3) Cubist__
```{r}

set.seed(042760)
library(Cubist)
cubist <- cubist(dfTrain2, yTrain)
cubist

cubistPred <- predict(cubist,  newdata = dfTest2)
postResample(pred = cubistPred, obs = yTest) 
```

The boosted models performs best on the test data.

__(b) Which predictors are most important in the optimal tree-based regression  model? Do either the biological or process variables dominate the list?  How do the top 10 important predictors compare to the top 10 predictors  from the optimal linear and nonlinear models?__

Manufacturing processes are more prominent in this model.  Also, one predictor dominates: MP32.


```{r}
head(summary.gbm(boosted),10)


```


__(c) Plot the optimal single tree with the distribution of yield in the terminal  nodes. Does this view of the data provide additional knowledge about the  biological or process predictors and their relationship with yield?__

The tree confirms the importance of MP32 in determining yield.  MP32 wasn't even in the top 10 in the non-tree models.  In addition, while manufacturing predictors dominate at each level, biological predictors are also present.  There is really only one route from root node to end which follows manufacturing indicators exclusively.

```{r}
set.seed(042760)
library(rpart)
library(rpart.plot)

rpart1 <- rpart(Yield ~., data = dfChem3)
rpart.plot(rpart1)

```

