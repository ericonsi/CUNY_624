---
title: "Eric_Hirsch_624_Homework_8"

output:
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r}

library(tidyverse)
library(EHData)
library(gridExtra)
library(caret)
library(AppliedPredictiveModeling)
library(randomForest)
library(Cubist)

```

8.1, 8.2, 8.3, 8.7

#### 8.1. Recreate the simulated data from Exercise 7.2:  

```{r}

library(mlbench)  
set.seed(200)  
simulated <- mlbench.friedman1(200, sd = 1) 
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated) 
colnames(simulated)[ncol(simulated)] <- "y"  
```

__(a) Fit a random forest model to all of the predictors, then estimate the  variable importance scores:__ 

```{r}
library(randomForest)
model1 <- randomForest(y ~., data = simulated, importance = TRUE, ntree = 1000)
rfImp1 <- varImp(model1, scale = FALSE) 
rfImp1
```


__Did the random forest model significantly use the uninformative predictors (V6 â€“ V10)?__

The predictors V6 through V10 have varimps that are either very low or negative.  

__(b) Now add an additional predictor that is highly correlated with one of the  informative predictors. For example:__

```{r}

simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1
cor(simulated$duplicate1, simulated$V1)  

```

```{r}

library(randomForest)
model2 <- randomForest(y ~., data = simulated, importance = TRUE, ntree = 1000)
rfImp1 <- varImp(model2, scale = FALSE) 
rfImp1

```


__Fit another random forest model to these data. Did the importance score  for V1 change? What happens when you add another predictor that is  also highly correlated with V1?__ 

The varimp is split between the two predictors. V1 drops and Duplicate1 picks up the slack.

__(c) Use the cforest function in the party package to fit a random forest model  using conditional inference trees. The party package function varimp can  calculate predictor importance. The conditional argument of that function toggles between the traditional importance measure and the modified  version described in Strobl et al. (2007). Do these importances show the  same pattern as the traditional random forest model?__

```{r}
library(party)
set.seed(042760)
cm <- cforest(y ~ ., data = simulated)
```

```{r}

varimp(cm)
varimp(cm, conditional = TRUE)

```

The pattern is roughly the same.

__(d) Repeat this process with different tree models, such as boosted trees and  Cubist. Does the same pattern occur?__
```{r}

library(gbm)
boosted <- gbm(y ~., data = simulated)
summary.gbm(boosted)


```

```{r}

library(Cubist)

simulated1 <- simulated %>%
  dplyr::select(-y)

target <- simulated[,"y"]

c1 <- cubist(x = simulated1, y=target, committees = 100)
varImp(c1)

```


The two models show the same pattern except that the duplicate column is not used in the cubist model.

#### 8.2. Use a simulation to show tree bias with different granularities.

```{r}
set.seed(042760) 

high1 <- sample(0:1000, 500, replace = T)
high2 <- sample(0:1000, 500, replace = T)
high3 <- sample(0:1000, 500, replace = T)
target = high1 + 2*high2 + 5*high3 + rnorm(500, sd=500)

dfHigh <- data.frame(high1, high2, high3, target, stringsAsFactors = FALSE)

low1 <- sample(0:100, 500, replace = T)
low2 <- sample(0:100, 500, replace = T)
low3 <- sample(0:100, 500, replace = T)
target = low1 + 2*low2 + 5*low3 + rnorm(500, sd=50)

dfLow <- data.frame(low1, low2, low3, target, stringsAsFactors = FALSE)

```


```{r}

model3 <- randomForest(target ~., data = dfHigh, importance = TRUE)
rfImp3 <- varImp(model3, scale = FALSE) 
model3
rfImp3

model4 <- randomForest(target ~., data = dfLow, importance = TRUE)
rfImp4 <- varImp(model4, scale = FALSE) 
model4
rfImp4
```

I used two dataframes of simulated data, each with 3 variables and a y generated using the same formula from the 3 variables plus a random variable.  The first dataframe chose values form 1 to 100 (high) and the second from 1 to 100 (low).  The high granularity model explained slightly more of the variance than the low granularity.

#### 8.3. In stochastic gradient boosting the bagging fraction and learning rate  will govern the construction of the trees as they are guided by the gradient. Although the optimal values of these parameters should be obtained  through the tuning process, it is helpful to understand how the magnitudes  of these parameters affect magnitudes of variable importance. Figure 8.24  provides the variable importance plots for boosting using two extreme values  for the bagging fraction (0.1 and 0.9) and the learning rate (0.1 and 0.9) for  the solubility data. The left-hand plot has both parameters set to 0.1, and  the right-hand plot has both set to 0.9:  

__(a) Why does the model on the right focus its importance on just the first few  of predictors, whereas the model on the left spreads importance across  more predictors?__

When both parameters are set high the model will have a tendency to overfit.  It is interesting to note that an overfit model in a boosting algorithm may result in a model with fewer predictors at higher levels of importance.  In some models, as we begin to overfit we may pick up small effects in the training data that add features to our model.  Presumably because boosting is iterative, overfitting drops out features as high importance features take over,

__(b) Which model do you think would be more predictive of other samples?__

I would presume the left hand model.  On the other hand, a model with a low bagging fraction might underfit the data.

__(c) How would increasing interaction depth affect the slope of predictor importance for either model in Fig. 8.24?__

Interaction depth specifies the maximum depth for each tree (starting from a single node).  Increasing the depth also works in the direction of overfitting.  Therefore, the tendency will be toward fewer predictors with higher importance.

#### 8.7. Refer to Exercises 6.3 and 7.5 which describe a chemical manufacturing  process. Use the same data imputation, data splitting, and pre-processing  steps as before and train several tree-based models:  

```{r}


data(ChemicalManufacturingProcess)

dfChem <- ChemicalManufacturingProcess
dfChem2 <- EHPrepare_MissingValues_Imputation(dfChem, impute = "median")
dfChem3 <- EHPrepare_ScaleAllButTarget(dfChem2, "Yield")

set.seed(042760)

i <- createDataPartition(dfChem3[,1], p = .8, list=FALSE)

dfTrain <- dfChem3[i, ]
dfTest <- dfChem3[-i, ]

dfTrain2 <- dfTrain[,-58]
dfTest2 <- dfTest[,-58]
yTrain <- dfTrain[,58]
yTest <- dfTest[,58]

trainingData$x <- dfTrain2
trainingData$y <- yTrain
testData$x <- dfTest2
testData$y <- yTest



```

__(a) Which tree-based regression model gives the optimal resampling and test  set performance?__

```{r}
set.seed(042760)
model1 <- randomForest(Yield ~., data = dfTrain, importance = TRUE, ntree = 1000)
rfImp1 <- varImp(model1, scale = FALSE) 
rfImp1

rfPred <- predict(model1, newdata = testData$x)
postResample(pred =rfPred, obs = testData$y) 
```
```{r}
set.seed(042760)
library(gbm)
boosted <- gbm(Yield ~., data = dfTrain)
summary.gbm(boosted)


boostPred <- predict(boosted, newdata = testData$x)
postResample(pred =boostPred, obs = testData$y) 
```

```{r}

set.seed(042760)
library(Cubist)
cubist <- cubist(trainingData$x, trainingData$y)
cubist

cubistPred <- predict(cubist,  newdata = testData$x)
postResample(pred = cubistPred, obs = testData$y) 
```

The boosted models performs best on the test data.

__(b) Which predictors are most important in the optimal tree-based regression  model? Do either the biological or process variables dominate the list?  How do the top 10 important predictors compare to the top 10 predictors  from the optimal linear and nonlinear models?__

```{r}

head(varImp(cubist),10)

```


__(c) Plot the optimal single tree with the distribution of yield in the terminal  nodes. Does this view of the data provide additional knowledge about the  biological or process predictors and their relationship with yield?__

