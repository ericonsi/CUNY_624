---
title: "Eric_Hirsch_624_Homework_4"
output: html_document
date: "2023-02-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r}

library(tidyverse)
library(tsibbledata)
library(tsibble)
library(forecast)
library(fpp3)
library(fable)
library(EHData)
library(gridExtra)

```

8.1, 8.5, 8.6, 8.7, 8.8, 8.9


__*3.1. The UC Irvine Machine Learning Repository contains a data set related  to glass identification.*__

__*3.1a Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.*__

There are 9 variables altogether, including 8 numeric and one factor (the target, a six level variable).  

```{r}
library(mlbench)
data(Glass)

summary(Glass)
str(Glass)

dfGlass <- Glass %>%
  mutate(Type = as.numeric(Type))

```
The heatmap shows some multicollinearity.  However, the RI/CA and MG/Type pairs are the only ones with a correlation above .6 or below -.6.

```{r}

a <- EHExplore_Multicollinearity(dfGlass, threshold=.6, printHighest = TRUE)
```


__*3.1b Do there appear to be any outliers in the data? Are any predictors skewed?*__

Boxplots show possible outliers for K, Fe, Na and possibly others.  Without knowing more about the data we cannot know whether they are anomalies or data errors.  Distributions vary - many are only somewhat skewed while others (K, Ba, Fe, Mg)  are highly skewed or are bimodal.

```{r}

w <- EHSummarize_SingleColumn_Boxplots(dfGlass)
x <- EHSummarize_SingleColumn_Histograms(dfGlass)

grid.arrange(grobs=w[1:10], ncols=3)
grid.arrange(grobs=x[1:10], ncols=3)

```

__*3.1c (c) Are there any relevant transformations of one or more predictors that  might improve the classification model?*__

We can run a multinomial regression to get a baseline.  The AIC is 399.

```{r}
library(caret)

 library("nnet")
test <- multinom(Type ~ ., data = dfGlass)
test

```
We can try to normalize the skewed distributions using boxcox. Using trial and error, we find that 3 transformations (al, Ba and Ca) reduce our AIC to 377.  It is worth noting that when all four of the mostly skewed variables are transformed, the AIC increases to 401.  Thus, boxcox transformations are no guarantee of improved fit. 

```{r}
#devtools::install_github("ericonsi/EHData")
#library(EHData)

Z3 <- dfGlass %>%
  mutate(Ba=Ba+1, K=K+1, Fe=Fe+1, Mg=Mg+1)

a <- BoxCoxTrans(Z3$Ba)$lambda
b <- BoxCoxTrans(Z3$K)$lambda
c <- BoxCoxTrans(Z3$Fe)$lambda
d <- BoxCoxTrans(Z3$Mg)$lambda
e <- BoxCoxTrans(Z3$Al)$lambda
f <- BoxCoxTrans(Z3$Ca)$lambda

Z4 <- Z3 %>%
  #mutate(Ba=BoxCox(Ba, a), K=BoxCox(K, b), Fe=BoxCox(Fe, c), Mg=BoxCox(Mg, d))
    mutate(Al=BoxCox(Al, e), Ba=BoxCox(Ba, a), Ca =BoxCox(Ca, f))

hist(dfGlass$Al)
hist(Z4$Al)

test <- multinom(Type ~ ., data = Z4)
test


```
Removing possible outliers reduces our AIC to 374. (We should take care in removing this data, which might tell us something interesting when data meets certain rare conditions.)

```{r}
dfGlass2 <- dfGlass %>%
  filter(K<4, Fe<.5, Na <15) %>%
  mutate(Ba=Ba+1, K=K+1, Fe=Fe+1, Mg=Mg+1) %>%
  mutate(Al=BoxCox(Al, e), Ba=BoxCox(Ba, a), Ca =BoxCox(Ca, f))

test <- multinom(Type ~ ., data = dfGlass2)
test


```

__*3.2 The soybean data can also be found at the UC Irvine Machine Learning  Repository. Data were collected to predict disease in 683 soybeans.*__

__*3.2a Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?*__

There are many predictors that are severely unbalanced.  For example, in the case of mycelium, 0's outweigh 1's by a factor of more than 100.  Other variables have balance issues as well, though not as sever - Sclerotia is unbalanced and there are possible degeneration issues for mold.growth, seed.discolor, seed.size and shriveling as well.

```{r}
library(mlbench)
data(Soybean)

dfSoybean <- Soybean 

summary(dfSoybean)


```
__*3.2b Roughly 18% of the data are missing. Are there particular predictors that  are more likely to be missing? Is the pattern of missing data related to  the classes?*__

There are 2337 missing values in the dataset. The data are missing in groups.  For example, columns related to the leaf (e.g., leaf.halo and leaf.marg) are all missing or not as a group. Many records are missing most of the groups.

```{r}
sum(is.na(dfSoybean))
```


```{r}

EHSummarize_MissingValues(dfSoybean)
```

The records with missing values are highly correlated with the various classes as the following mutlinomial regression shows.  

```{r}
dfSoybean1 <-dfSoybean %>%
  mutate(missing = ifelse(if_any(.fns = is.na),1,0), random=ifelse(nrow(dfSoybean)>300,1,0)) 

test <- multinom(Class ~ missing, data = dfSoybean1)
test

z <- summary(test)$coefficients/summary(test)$standard.errors
p <- (1 - pnorm(abs(z), 0, 1)) * 2
p
```

__*3.2c Develop a strategy for handling missing data, either by eliminating  predictors or imputation.*__

We cannot eliminate records with any missing vaues because there are too many, and because they are not missing at random (MNAR). Some of the columns are missing 18% of their values, but eliminating them risks losing valuable information.  In some cases, degenerate columns overlap with missing value columns (e.g., mold.growth) - these might be removed because there is more than one reason to do so.

Otherwise we are looking at imputation. Prior to imputation, dummy variables should be created to flag the missing group(s) to which the record belongs.  Then we should test individual columns to see if they are MNAR - if not, we can use KNN or Linear regression to estimate missing values.  When the data is MNAR there is no reason to think the variables are necessarily predictable from the nonmissing values - in this case it might be best to us median or mean.

