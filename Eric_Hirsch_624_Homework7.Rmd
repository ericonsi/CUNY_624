---
title: "Eric_Hirsch_624_Homework_7"

output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r}

library(tidyverse)
library(EHData)
library(gridExtra)
library(caret)
library(AppliedPredictiveModeling)

```

6.2, 6.3

```{r}


data(permeability)  

dfPermeability <- as.data.frame(permeability)
dfFingerprints <- as.data.frame(fingerprints)

summary(dfPermeability)
str(dfPermeability)

```


__*6.2b Filter out the predictors that have low frequencies using the nearZeroVar function from the caret package.  How many predictors are left for modeling?*__

The number of predictors drop from 1107 to 388.

```{r}
dfFingerprints2 <- dfFingerprints[, -nearZeroVar(dfFingerprints)]
dfAll <- cbind(dfFingerprints2, dfPermeability)

```


__*6.12c Split the data into a training and a test set, pre-process the data, and tune a PLS model. How many latent variables are optimal and what is the corresponding resampled estimate of R2?*__

First, some investigation.  There are no missing values.  Multicollinearity is a big issue here - there are 152 factors pairs that have correlations of 1. If we were to model this data properly we would remove many of these predictors. The following are just the first 10 of them.:

```{r}

a <- EHExplore_Multicollinearity(threshold = .99, dfFingerprints2, printHeatMap=FALSE, printHighest = FALSE)

head(as.data.frame(a[2]), 10)

```

PCA demonstrates that 95% of the variance is explained by only 27 components. 

```{r}


pca <- prcomp(dfFingerprints2, center = TRUE, scale. = TRUE)
dfX <- as.data.frame(summary(pca)$importance[2,]) |>
  mutate(Component=row_number()) |>
  dplyr::rename(Variance = 1) |>
  filter(row_number()<28) 

ggplot(dfX, aes(Component, Variance)) +
  geom_line() + ggtitle("Components by Variance Explained")

```

Now we separate the data into train and test sets and tune a PLS model.

```{r}

set.seed(042760)

library(pls)
i <- createDataPartition(dfAll[,389], p = .8, list=FALSE)

dfTrain_IncludesY <- dfAll[i, ]
dfTest_IncludesY <- dfAll[-i, ]

dfTrain_ExcludesY <- dfTrain_IncludesY[,-389]
dfTest_ExcludesY <- dfTest_IncludesY[,-389]

yTrain <- dfTrain_IncludesY[,389]
yTest <- dfTest_IncludesY[,389]

ctrl <- trainControl(method = "cv", number = 10)

model2 <- train(dfTrain_ExcludesY, yTrain, method = "pls", metric='Rsquared', tuneLength=50, trControl = ctrl, preProc = c("center", "scale"))

summary(model2)
plot(model2) 
model2

```

The highest R2 is .45, with the number of components considered only 5.  

6.d (d) Predict the response for the test set. What is the test set estimate of R2? 

Oddly, the model performed better on the test set  R Squared = .50.

```{r}

set.seed(042760)

predictions <- as.data.frame(predict(model2, dfTest_ExcludesY, ncomp=5))
pred2 <- cbind(predictions, yTest) %>%
  dplyr::rename(predictions = 1, actual=2) %>%
  mutate(residSQ = (predictions-actual)^2)

ymean = mean(pred2$actual)

pred3 <- pred2 |>
  mutate(SquaresTot = (actual-ymean)^2)
  

R2 <- 1 - sum(pred3$residSQ)/sum(pred3$SquaresTot)

R2

```
__*3.2e Try building other models discussed in this chapter. Do any have better  predictive performance?  Would you recommend any of your models to replace the permeability  laboratory experiment? *__. 

We use LASSO. It performs better with an R squared  of .60.  I would use this model instead because of the better R Squared.

```{r}
set.seed(042760)
library(glmnet)
xxTrain <- data.matrix(dfTrain_ExcludesY)
xxTest <- data.matrix(dfTest_ExcludesY)

mcv <- cv.glmnet(xxTrain, yTrain, alpha = 1)

lambda1 <- mcv$lambda.min

m10 <- glmnet(xxTrain, yTrain, alpha = 1, lambda = lambda1)
summary(m10)

```

```{r}
set.seed(042760)
predictions <- as.data.frame(predict(m10, xxTest))
pred2 <- cbind(predictions, yTest) %>%
  dplyr::rename(predictions = 1, actual=2) %>%
  mutate(residSQ = (predictions-actual)^2)

ymean = mean(pred2$actual)

pred3 <- pred2 |>
  mutate(SquaresTot = (actual-ymean)^2)
  

R2 <- 1 - sum(pred3$residSQ)/sum(pred3$SquaresTot)

R2

```


6.3

The matrix processPredictors contains the 57 predictors (12 describing  the input biological material and 45 describing the process predictors)  for the 176 manufacturing runs. Yield contains the percent yield for each run. 


```{r}
data(ChemicalManufacturingProcess)

dfChem <- ChemicalManufacturingProcess
```

(b) A small percentage of cells in the predictor set contain missing values. Use  an imputation function to fill in these missing values (e.g., see Sect. 3.8).  

The number of missing values is low.  Furthermore, variances for many of the columns is low.  We therefore use mean imputation for the missing values.

The data is marked by a great degree of multicollinearity. For example, 10 predictor pairs have correlations of .95 or higher.

```{r}

EHSummarize_MissingValues(dfChem)
dfChem2 <- EHPrepare_MissingValues_Imputation(dfChem)

```

```{r}

EHExplore_Multicollinearity(dfChem2, printHeatMap = FALSE, threshold=.95, printHighest = TRUE)
```
(c) Split the data into a training and a test set, pre-process the data, and  tune a model of your choice from this chapter. What is the optimal value of the performance metric?  

We choose LASSO regression for our model.  LASSO has the advantage that it will eliminate unneeded indoicators, making for a more readable and intuitive model.

In order to compare models, we will use the RMSE, which gives us a sense of the distribution of errors our model is generating.

The RMSE on the training set is .40.

```{r}

dfChem3 <- EHPrepare_ScaleAllButTarget(dfChem2, "Yield")

set.seed(042760)

library(pls)
i <- createDataPartition(dfChem3[,1], p = .8, list=FALSE)

dfTrain <- dfChem3[i, ]
dfTest <- dfChem3[-i, ]

dfTrain2 <- dfTrain[,-1]
dfTest2 <- dfTest[,-1]
yTrain <- dfTrain[,1]
yTest <- dfTest[,1]


library(glmnet)
x <- data.matrix(dfTrain2)
xtest <- data.matrix(dfTest2)

mcv <- cv.glmnet(x, yTrain, alpha = 1)

lambda1 <- mcv$lambda.min

m10 <- glmnet(x, yTrain, alpha = 1, lambda = lambda1)

print("Train")
predictions <- predict(m10, as.matrix(dfTrain2))
sqrt(mean((predictions - yTrain)^2))

```

(d) Predict the response for the test set. What is the value of the performance  metric and how does this compare with the resampled performance metric  on the training set?  

On the test set, the RMSE is .57.  This is somewhat higher.

```{r}

print("Test")
predictions <- predict(m10, xtest)
sqrt(mean((predictions - yTest)^2))

```

(e) Which predictors are most important in the model you have trained? Do  either the biological or process predictors dominate the list?  

As expected, the LASSO model eliminates a number of predictors from the model.  In all, only 13 predictors are preserved. Biological predictors preserve the a higher percentage of predictors, and have the highest coefficents.  Therefore, they are more important. 

```{r}
library(broom)

coef(m10)
  
```

__*6.3f Explore the relationships between each of the top predictors and the response. How could this information be helpful in improving yield in future  runs of the manufacturing process?*__



```{r}


zz <- tidy(m10) %>%
  dplyr::select(term, estimate) |>
  arrange(desc(abs(estimate)))
zz
  
dfGo <- dfChem2 %>%
  dplyr::select(BiologicalMaterial01, BiologicalMaterial02, BiologicalMaterial08, ManufacturingProcess01, BiologicalMaterial03, Yield)

EHSummarize_StandardPlots(dfGo, "Yield")

```

