---
title: "Eric_Hirsch_624_Homework_8"

output:
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r}

library(tidyverse)
library(EHData)
library(gridExtra)
library(caret)
library(AppliedPredictiveModeling)

```

7.2, 7.5

#### 7.2 Friedman (1991) introduced several benchmark data sets created by simulation ... Tune several models on these data ... Which models appear to give the best performance? Does MARS select the  informative predictors (those named X1â€“X5)?

```{r}


library(mlbench)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)  
trainingData$x <- data.frame(trainingData$x)  
featurePlot(trainingData$x, trainingData$y)  
testData <- mlbench.friedman1(5000, sd = 1)  
testData$x <- data.frame(testData$x) 
```

__1. K-Nearest Neighbors__ 

```{r}

library(caret)
knnModel <- train(x = trainingData$x, y = trainingData$y, method = "knn", preProc = c("center", "scale"), tuneLength = 10)
knnModel  

knnPred <- predict(knnModel, newdata = testData$x)
postResample(pred = knnPred, obs = testData$y) 


```

__2. Neural Network__

Decay = .06, size = 5.

```{r}
set.seed(042760)
library(nnet)
nn <- nnet(trainingData$x, trainingData$y,
                size = 5,
                decay = 0.06,
                linout = TRUE,
                trace = FALSE,
                maxit = 500,
                MaxNWts = 5 * (ncol(trainingData$x) + 1) +5 +1)
nn

nnPred <- predict(nn, newdata = testData$x)
postResample(pred = nnPred, obs = testData$y)

```

__3. MARS__

```{r}
library(earth)
mars <- earth(trainingData$x, trainingData$y)
mars

marsPred <- predict(mars, newdata = testData$x)
postResample(pred = marsPred, obs = testData$y)

```

__4. SVM (linear and radial)__

```{r}
library(caret)

svm_Radial <- train(trainingData$x, trainingData$y,
                   method = "svmRadial",
                   preProc = c("center","scale"),
                   trControl = trainControl(method = "cv"))

svm_Radial

svmPredR <- predict(svm_Radial, newdata = testData$x)
postResample(pred = svmPredR, obs = testData$y)

print('')
print('___________________________________________')
print('')

svm_Linear <- train(trainingData$x, trainingData$y,
                   method = "svmLinear",
                   preProc = c("center","scale"),
                   tuneLength = 14,
                   trControl = trainControl(method = "cv"))

svm_Linear


svmPredL <- predict(svm_Linear, newdata = testData$x)
postResample(pred = svmPredL, obs = testData$y)


```

The models create RMSEs of between 1.5 and 3.2.  Given that the mean y value is 14 and the standard deviation is 5, an RMSE of 1.5 is relatively low.  This model (Neural Net) also had an R-Squared of .91, which also suggests a relatively reasonable fit.  MARS uses the fist six indicators in this case.

KNN is not generally considered a highly powerful model so it is not surprising that its predictive power is low.  NNet was very sensitive to decay.  At a decay of .06, the model outperforms MARS but at ,01 it does not.  The SVM models performed in between  - the radial was more accurate than the Linear.

```{r}

averageY <- mean(testData$y)
sdY <- sd(testData$y)

tab <- matrix(c("KNN", 3.2, .68, "NeuralNet", 1.5, .91, "MARS", 1.8, .87, "SVM-Linear", 2.8, .70,"SVM_Radial",2.1,.83), ncol=3, byrow=TRUE)
colnames(tab) <- c('Model','RMSE','RSquared')
tab <- as.table(tab)
tab

averageY 
sdY

```


#### 7.5. Exercise 6.3 describes data for a chemical manufacturing process. Use  the same data imputation, data splitting, and pre-processing steps as before  and train several nonlinear regression models.  (a) Which nonlinear regression model gives the optimal resampling and test  set performance?  

```{r}

data(ChemicalManufacturingProcess)

dfChem <- ChemicalManufacturingProcess
dfChem2 <- EHPrepare_MissingValues_Imputation(dfChem, impute = "median")
dfChem3 <- EHPrepare_ScaleAllButTarget(dfChem2, "Yield")

set.seed(042760)

i <- createDataPartition(dfChem3[,1], p = .8, list=FALSE)

dfTrain <- dfChem3[i, ]
dfTest <- dfChem3[-i, ]

dfTrain2 <- dfTrain[,-1]
dfTest2 <- dfTest[,-1]
yTrain <- dfTrain[,1]
yTest <- dfTest[,1]

trainingData$x <- dfTrain2
trainingData$y <- yTrain
testData$x <- dfTest2
testData$y <- yTest




```
__1. KNN __

```{r}

library(caret)
knnModel <- train(x = trainingData$x, y = trainingData$y, method = "knn", preProc = c("center"), tuneLength = 10)
knnModel  


```

```{r}

knnPred <- predict(knnModel, newdata = testData$x)
postResample(pred = knnPred, obs = testData$y) 

```
__2. Neural Network__

Decay = .03, size = 4.

```{r}
library(nnet)
set.seed(042762)
nn <- nnet(trainingData$x, trainingData$y,
                size = 4,
                decay = 0.03,
                linout = TRUE,
                trace = FALSE,
                maxit = 500,
                MaxNWts = 5 * (ncol(trainingData$x) + 1) +5 +1)

nnPred <- predict(nn, newdata = testData$x)
postResample(pred = nnPred, obs = testData$y)

```

__3. MARS__

```{r}
set.seed(042762)
library(earth)
mars <- earth(trainingData$x, trainingData$y)
mars

marsPred <- predict(mars, newdata = testData$x)
postResample(pred = marsPred, obs = testData$y)

```

__4. SVM__

```{r}
library(caret)

svm_Radial <- train(trainingData$x, trainingData$y,
                   method = "svmRadial",
                   preProc = c("center"),
                   trControl = trainControl(method = "cv"))

svm_Radial

svmPredR <- predict(svm_Radial, newdata = testData$x)
postResample(pred = svmPredR, obs = testData$y)

print('')
print('___________________________________________')
print('')

svm_Linear <- train(trainingData$x, trainingData$y,
                   method = "svmLinear",
                   preProc = c("center","scale"),
                   trControl = trainControl(method = "cv"))

svm_Linear


svmPredL <- predict(svm_Linear, newdata = testData$x)
postResample(pred = svmPredL, obs = testData$y)


```

The models create RMSEs of between .56 and .65 (and an outlier .95 for linear SVM).  Given that the mean y value is about 0 and the standard deviation is 1.15, an RMSE of 5.6 is reasonable. This model (again Neural Net) also had an R-Squared of .80, which also suggests a relatively reasonable fit.

With the exception of KNN and linear SVM, all of the models are within a small range. The Lasso model perfomed for homework 7 is also a strong contender, with very comparable scores to the neural net.  


```{r}

averageY <- mean(testData$y)
sdY <- sd(testData$y)

tab <- matrix(c("KNN", .65, .69, "NeuralNet", .56, .80, "MARS", .58, .75, "SVM-Linear", .95, .40,"SVM_Radial",.60,.75, "Lasso", .57, .80), ncol=3, byrow=TRUE)
colnames(tab) <- c('Model','RMSE','RSquared')
tab <- as.table(tab)
tab

averageY 
sdY

```

__(b) Which predictors are most important in the optimal nonlinear regression model? Do either the biological or process variables dominate the  list? How do the top ten important predictors compare to the top ten  predictors from the optimal linear model?__  

The neural net model has more processing factors at the forefront than the Lasso model.  While the results are similar, there is little overlap between the sets of indicators chosen by the two models.

```{r}
set.seed(042760)
svmImp <- varImp(nn) %>%
  arrange(desc(abs(Overall)))
head(svmImp,10)



```




__(c) Explore the relationships between the top predictors and the response for  the predictors that are unique to the optimal nonlinear regression model.  Do these plots reveal intuition about the biological or process predictors and their relationship with yield?__

Most of the biological indicators are highly correlated with yield, however the manufacturing indicators the r is low and the p is high.  This suggests that the model finds hidden relationships more important than individual correlations  Because of the high degree of multicollinearity, the model is looking for the unique set of indicators which explain variance, which may be certain indicators only in combination with other indicators.

```{r}

dfGo <- dfChem2 %>%
  dplyr::select(BiologicalMaterial10, BiologicalMaterial01, ManufacturingProcess05, ManufacturingProcess43, ManufacturingProcess20, Yield)

EHSummarize_StandardPlots(dfGo, "Yield")

```
