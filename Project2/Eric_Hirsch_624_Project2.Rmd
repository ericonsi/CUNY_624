---
title: "Eric_Hirsch_624_Project2"

output:
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r}

library(tidyverse)
devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)
library(gridExtra)
library(caret)
library(AppliedPredictiveModeling)
library(randomForest)
library(Cubist)
library(readxl)

```
```{r}
dfPH <- as.data.frame(read_excel("D:\\RStudio\\CUNY_624\\Project2\\StudentData.xlsx"))
dfPH_Eval <- as.data.frame(read_excel("D:\\RStudio\\CUNY_624\\Project2\\StudentEvaluation.xlsx"))

ChangeNA <- function(df){
  df <- df |>
    mutate('Brand Code' = ifelse(is.na(df$'Brand Code'),"NA",df$'Brand Code'))
  return (df)
}

dfPHa <- ChangeNA(dfPH)
dfPH_Evala <- ChangeNA(dfPH_Eval)
```
#### Introduction

ABC Beverage is facing new regulations requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Below is the technical report detailing our examination of the data and creation of our PH model.

#### Data Summary and Preparation

The data consists of 2571 beverage records, including PH (the target) and 32 possible predictors of PH. 31 of the predictors are numeric, and one (Brand) is character.  Many of the predictors have missing values.


```{r}

summary(dfPHa)
str(dfPHa)
```

```{r}
dfPH_Num <- dfPHa |>
  dplyr::select(-"Brand Code")
```

#### Missing Values

A number of predictors have missing values; however, the only one of significance is MFR (8%).  The missing values do not appear to be correlated with each other, and records with missing MFR do not appear to be correlated with PH.  We therefore conclude that missingness is a relatively random phenomenon in this dataset and do a simple median imputation for all missing values.

There are 4 missing target values as well - however, there are so few that we decide to impute them rather than lose them from the database.

For the only character predictor, Brand, we will create a variable Brand_NA when we dummify it.

```{r}
EHSummarize_MissingValues(dfPH_Num)

mfr_flag <- ifelse(is.na(dfPH_Num$MFR),1,0)

print("Correlation between MFR missingness and PH")
cor.test(dfPH_Num$PH, mfr_flag)

```

```{r}
dfPH1 <- EHPrepare_MissingValues_Imputation(dfPHa, impute="median")
dfPH_Eval1 <- EHPrepare_MissingValues_Imputation(dfPH_Evala, impute="median")
```

### Multicollinearity

There is some degree of multicollinearity in the database. In particular, Density, Balling, Balling level and Alch Rel are all highly correlated with each other.  While multicollinearity does not necessarily interfere with predictive power, it does impede inference.  Since we need to not only predict PH but understand our model, we will remove Balling, since Balling level and Density carry most of its information.

```{r}
dfPH_Num1 <- dfPH1 |>
  dplyr::select(-"Brand Code")

a <- EHExplore_Multicollinearity(dfPH_Num1, threshold=.85, printHighest = TRUE)

```

```{r}

dfPH2 <- dfPH1 |>
  dplyr::select(-Balling)
dfPH_Eval2 <- dfPH_Eval1 |>
  dplyr::select(-Balling)

```

```{r}


dfPH3 <- EHPrepare_CreateDummies(dfPH2, "PH")
dfPH_Eval3 <- EHPrepare_CreateDummies(dfPH_Eval2, "PH")

```


#### Outliers, Distributions and Correlations 

PH is normally-enough distributed so we will not transform it at this point.  We see some skewed distributions among some of the predictors and may need to correct for that depending on our algorithm choice.  

A number of factors have a wealth of disturbing zeroes (PSC, HydPressure1, etc.). Sometimes zeroes are a stand in for NA.  In a real world case we would use domain knowledge to assess the validity of these zeroes - however, since we don't know, we will assume they are legitimate values. While there are high and low values, these values come in groups - therefore, we will not remove outliers but we will run a regression to check for influential points.

A fair number of factors are correlated with PH - this bodes well for a predictive model.

```{r}

a <- EHSummarize_StandardPlots(dfPH3, "PH")

```

#### Exploratory Regression Model

We run an ordinary least square regression to gain insight into our data.  We use the Step AIC algorithm which eliminates predictors based on improvements to AIC.

A large number of factors remain in the model.  R-Squared is 41% - we hope to improve this with more powerful predictive algorithms. The model shows heteroskedasticity, so we will perform a boxcox on PH. The QQ plot suggests a relatively normally distributed target.

There are 3 influential records.  We will remove them.


```{r}

library(MASS)

set.seed(042760)
a <-EHModel_Regression_StandardLM(dfPH3, "PH", xseed="042760", returnLM=TRUE)

```


```{r}
#devtools::install_github("ericonsi/EHData", force=TRUE)
library(EHData)

dfPH4a <- EHPrepare_RemoveRecordsByRowNumber(dfPH3, c(1094, 690, 2359))
xformula=PH~1

df <- dfPH4a
col="PH"
  
  hist(df[,col], main=paste(col, "- Before"))
  
  b <- boxcox(lm(PH~1., dfPH4a))
  lambda <- b$x[which.max(b$y)]
  df[, col] <- (df[,col] ^ lambda - 1) / lambda
  xq <- df[,col]
  
  hist(df[,col], main=paste(col, "- After, lambda =", lambda))

dfPH4 <- dfPH4a %>%
  mutate(PH=xq)

#write.csv(dfPH4,file="D:\\RStudio\\CUNY_624\\Project2\\Data.txt")
```

#### Machine Learning Models

We now test our data on 11 machine learning algorithms in order to determine the best predictive model.

```{r}

set.seed(042760)

i <- createDataPartition(dfPH4[,1], p = .8, list=FALSE)

dfTrain <- dfPH4[i, ]
dfTest <- dfPH4[-i, ]

dfTrain2 <- dfTrain[,-24]
dfTest2 <- dfTest[,-24]
yTrain <- dfTrain[,24]
yTest <- dfTest[,24]

```


__1). Lasso__

```{r}
library(glmnet)
x <- data.matrix(dfTrain2)
xtest <- data.matrix(dfTest2)

mcv <- cv.glmnet(x, yTrain, alpha = 1)

lambda1 <- mcv$lambda.min

m10 <- glmnet(x, yTrain, alpha = 1, lambda = lambda1, preProc = c("center", "scale"))
m10

print("Test Metrics:")
lassoPred <- predict(m10, xtest)
postResample(pred =lassoPred, obs = yTest) 

```

__2) Ridge__
```{r}

library(glmnet)
x <- data.matrix(dfTrain2)
xtest <- data.matrix(dfTest2)

mcv <- cv.glmnet(x, yTrain, alpha = 0)

lambda1 <- mcv$lambda.min

m10 <- glmnet(x, yTrain, alpha = 0, lambda = lambda1, preProc = c("center", "scale"))
m10

print("Test Metrics:")
ridgePred <- predict(m10, xtest)
postResample(pred =ridgePred, obs = yTest) 

```

__3). PLS__

```{r}

ctrl <- trainControl(method = "cv", number = 10)

model2 <- train(dfTrain2, yTrain, method = "pls", metric='Rsquared', tuneLength=50, trControl = ctrl, preProc = c("center", "scale"))
model2

print("Test Metrics:")
plsPred <- predict(model2, dfTest2)
postResample(pred =plsPred, obs = yTest) 

```

__4.) KNN__
```{r}
set.seed(042760)
knnModel <- train(x = dfTrain2, y = yTrain, method = "knn", preProc = c("center", "scale"), tuneLength = 10)
knnModel

print("Test Metrics:")
knnPred <- predict(knnModel, newdata = dfTest2)
postResample(pred = knnPred, obs = yTest) 

```

__5). Neural Net__

Decay = .04, size = 3.
```{r}
set.seed(042760)
library(nnet)
nn <- nnet(dfTrain2, yTrain,
                size = 3,
                decay = 0.04,
                linout = TRUE,
                trace = FALSE,
                maxit = 5000,
                MaxNWts = 5 * (ncol(dfTrain2) + 1) +5 +1)
nn

print("Test Metrics:")
nnPred <- predict(nn, newdata = dfTest2)
postResample(pred = nnPred, obs = yTest)


```

__6.) MARS__
```{r}
set.seed(042760)
library(earth)
mars <- earth(dfTrain2, yTrain)
mars

print("Test Metrics:")
marsPred <- predict(mars, newdata = dfTest2)
postResample(pred = marsPred, obs = yTest)
```

__7.) and 8.) SVM (linear and radial)__

```{r}
set.seed(042760)
library(caret)

svm_Radial <- train(dfTrain2, yTrain,
                   method = "svmRadial",
                   preProc = c("center","scale"),
                   trControl = trainControl(method = "cv"))

svm_Radial

print("Test Metrics:")
svmPredR <- predict(svm_Radial, newdata = dfTest2)
postResample(pred = svmPredR, obs = yTest)

print('')
print('___________________________________________')
print('')

svm_Linear <- train(dfTrain2, yTrain,
                   method = "svmLinear",
                   preProc = c("center","scale"),
                   tuneLength = 14,
                   trControl = trainControl(method = "cv"))

svm_Linear

print("Test Metrics:")
svmPredL <- predict(svm_Linear, newdata = dfTest2)
postResample(pred = svmPredL, obs = yTest)


```

__9.) Random Forest__

```{r}
set.seed(042760)
rf <- randomForest(PH ~., data = dfTrain, importance = TRUE, ntree = 1000)
rf

print("Test Metrics:")
rfPred <- predict(rf, newdata = dfTest2)
postResample(pred =rfPred, obs = yTest) 
```

__10.) GBM__

Depth = 4, ntrees = 10000, shrinkage = .007
```{r}
set.seed(042760)
library(gbm)
boosted <- gbm(
  formula = PH ~., data = dfTrain,
  distribution = "gaussian",
  n.trees = 10000,
  interaction.depth = 4,
  shrinkage = 0.007,
  cv.folds = 10,
  n.cores = NULL, 
  verbose = FALSE
  ) 
boosted
print(paste("Number of trees:", boosted$n.trees))

print("Test Metrics:")
boostPred <- predict(boosted, n.trees = boosted$n.trees, newdata = dfTest2)
postResample(pred =boostPred, obs = yTest) 
```


__11.) Cubist__
```{r}

set.seed(042760)
library(Cubist)
cubist <- cubist(dfTrain2, yTrain)
cubist

print("Test Metrics:")
cubistPred <- predict(cubist,  newdata = dfTest2)
postResample(pred = cubistPred, obs = yTest) 
```
#### Choosing the best model

The table below shows results for the 11 algorithms plus OLS.  Random Forest is clearly the best performer.

```{r}

tab <- matrix(c("K", "OLS Regression", 1.15, .43, "H", "Lasso", 1.13, .43, "J", "Ridge", 1.14, .43, "I", "PLS", 1.13, .43, "E", "KNN", 1.06, .50, "D", "NeuralNet", 1.05, .51, "G","MARS", 1.11, .45, "L", "SVM-Linear", 1.15, .38, "C", "SVM_Radial",.99,.57, "A", "Random Forest", .84, .72, "B", "GBM", .90, .64, "F", "Cubist", 1.13, .48), ncol=4, byrow=TRUE) 
colnames(tab) <- c("Rank",'Model','RMSE','RSquared')

x <- tab[order(tab[,"Rank"],decreasing=FALSE),]

tab <- as.table(x)
tab

```

#### Assessing variable importance

First we re-run our regression with scaled factors to get a baseline.

```{r}

dfPH4z <- EHPrepare_ScaleAllButTarget(dfPH4, "PH")
aa <- EHModel_Regression_StandardLM(dfPH4z, "PH", xseed="042760", returnLM = TRUE, tests = FALSE, vif = FALSE)

```

We compare the top ten factors in our regression with those of Random Forest, SVM_Radial and GBM and look for patterns.

```{r}

print("Top 10 regression coefficients")
  rimp <- as.data.frame(aa$coefficients)
  colnames(rimp)[1] = "Overall"

  rimp <- rimp |>
    dplyr::arrange(desc(abs(Overall)))
  head(rimp, 10)


TopTenVarImp <- function(m)
{
  
  imp <- as.data.frame(varImp(m, scale=FALSE)) |>
    dplyr::arrange(desc(Overall))
  head(imp, 10)
  
}
print("Top 10 Random Forest factors")
  TopTenVarImp(rf)
  
print("Top 10 GBM factors")
library(gbm)
par(mar = c(5, 8, 1, 1))
summary.gbm(
  boosted, 
  cBars = 10,
  method = relative.influence, # also can use permutation.test.gbm
  las = 2
  )

  
print("Top 10 SVM factors")
  srimp <- varImp(svm_Radial, scale = FALSE) 
  srimp <- srimp$importance |> dplyr::arrange(desc(Overall))
  head(srimp, 10)
```
When we examine the most important predictors, not only in our Random Forest model but in the other higher performing models as well, we find that Mnf.Flow is the most important predictor, followed by Usage.cont and Bowl.Setpoint.  Alch.Rel, Filler.level and Brand (especially Brand C) are also factors. 

Now we can use some tools that might give us insight into how these variables operate.

When we run an individual tree (see below) we can see that, at least for this tree, Mnf.Flow is the first node. While our Random Forest model aggregates many different trees, we can presume that many of them begin with Mnf.Flow.


```{r}

set.seed(042760)
library(rpart)
library(rpart.plot)

rpart1 <- rpart(PH ~., data = dfPH4)
rpart.plot(rpart1)

#rpart2 <- rpart(PH ~., data = dfPH4, minsplit=700)
#rpart.plot(rpart2)


```

We can also examine pdp plots.  In this case, we will use our boosted model (our second best model), because the tree already gives us insight with respect to the rf model.

```{r, figures-side, fig.show="hold", out.width="25%"}
#par(mfrow=c(2,2), mar = c(2, 2, .1, .1))
```
```{r}

library(pdp)

p1 <- boosted %>% 
partial(pred.var = "Mnf.Flow", n.trees=10000 ) %>%
plotPartial(smooth = TRUE, lwd = 2, ylab = expression(f(lstat)))

p2 <- boosted %>% 
partial(pred.var = "Usage.cont", n.trees=10000, ) %>%
plotPartial(smooth = TRUE, lwd = 2, ylab = expression(f(lstat)))

p3 <- boosted %>% 
partial(pred.var = "Oxygen.Filler", n.trees=10000, ) %>%
plotPartial(smooth = TRUE, lwd = 2, ylab = expression(f(lstat)))

p4 <- boosted %>% 
partial(pred.var = "Alch.Rel", n.trees=10000, ) %>%
plotPartial(smooth = TRUE, lwd = 2, ylab = expression(f(lstat)))

grid.arrange(p1, p2, p3, p4, ncol=2)
```

The pdp plots from the boosted model provide some confirmation of the tree model. Mnf.Flow breaks at -.50 and at -.100, Usage.cont at 23, and Alch.rel at 7.6. 

Pdp plots don't necessarily capture the affects of interactions.  When we re-examine the most important predictors from our rf model, we can see that they all have significant correlations with PH.  In addition, while the first three factors are also correlated with each other, the correlations are only about 50%. These are reassuring signs we have a robust model.  However, we would want to understand these interactions better before making further statements about variable importance.

```{r}

dfPH_Imp <- dfPH4 |>
  dplyr::select(PH, Mnf.Flow, Bowl.Setpoint, Usage.cont, Alch.Rel, Brand.Code_C )
a <- EHSummarize_StandardPlots(dfPH_Imp, "PH")


```

```{r}
q <- EHExplore_Multicollinearity(dfPH_Imp, threshold=.5, printHighest = TRUE)
```
#### Conclusion

We tested 11 machine learning on beverage data in order to predict PH. OUr final model is tree-based (Random Forest).  The Model R-Squared is 72%, which suggests that our predictions will tend to be in the ballpark but there is still quite a bit of room for small errors up and down.

```{r}

#In new model, would need to reverse the boxcox
#ww <- EHModel_Predict(rf, dfPH_Eval3, "PSC", writeFile = "Predictions.xlsx")


```




