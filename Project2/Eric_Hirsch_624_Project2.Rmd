---
title: "Eric_Hirsch_624_Project2"

output:
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r}

library(tidyverse)
devtools::install_github("ericonsi/EHData")
library(EHData)
library(gridExtra)
library(caret)
library(AppliedPredictiveModeling)
library(randomForest)
library(Cubist)
library(readxl)

```
```{r}
dfPH <- as.data.frame(read_excel("D:\\RStudio\\CUNY_624\\Project2\\StudentData.xlsx"))
dfPH_Eval <- as.data.frame(read_excel("D:\\RStudio\\CUNY_624\\Project2\\StudentEvaluation.xlsx"))

ChangeNA <- function(df){
  df <- df |>
    mutate('Brand Code' = ifelse(is.na(df$'Brand Code'),"NA",df$'Brand Code'))
  return (df)
}

dfPHa <- ChangeNA(dfPH)
dfPH_Evala <- ChangeNA(dfPH_Eval)
```
#### Introduction

ABC Beverage is facing new regulations requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Below is the technical report detailing our examination of the data and creation of our PH model.

#### Data Summary and Preparation

The data consistent of 2571 beverage records, including PH (the target) and 32 possible predictors of PH. 32 of the predictors are numeric, and one (Brand) is character.  Many of the predictors have missing values.

The exercise is to create a predictive model of PH based on this data.

```{r}

summary(dfPHa)
str(dfPHa)
```

```{r}
dfPH_Num <- dfPHa |>
  dplyr::select(-"Brand Code")
```

#### Missing Values

A number of predictors have missing values; however, the only one of significance is MFR (8%).  The missing values do not appear to be correlated with each other, and records with missing MFR do not appear to be correlated with PH.  We therefore conclude that missingness is a relatively random phenomenon in this dataset and do a simple median imputation for all missing values.

For the only character predictor, Brand, we will create a variable Brand_NA when we dummify it.

```{r}
EHSummarize_MissingValues(dfPH_Num)

mfr_flag <- ifelse(is.na(dfPH_Num$MFR),1,0)

print("Correlation between MFR missingness and PH")
cor.test(dfPH_Num$PH, mfr_flag)

```

```{r}
dfPH1 <- EHPrepare_MissingValues_Imputation(dfPHa, impute="median")
dfPH_Eval1 <- EHPrepare_MissingValues_Imputation(dfPH_Evala, impute="median")
```

### Multicollinearity

There is some degree of multicollinearity in the database. In particular, Density, Balling, Balling level and Alch Rel are all highly correlated with each other.  While multicollinearity does not necessarily interfere with predictive power, it does impede inference.  Since we need to not only predict PH but understand our model, we will remove Balling, since Balling level and Density carry most of its information.

```{r}
dfPH_Num1 <- dfPH1 |>
  dplyr::select(-"Brand Code")

a <- EHExplore_Multicollinearity(dfPH_Num1, threshold=.85, printHighest = TRUE)

```

```{r}

dfPH2 <- dfPH1 |>
  dplyr::select(-Balling)
dfPH_Eval2 <- dfPH_Eval1 |>
  dplyr::select(-Balling)

```

```{r}


dfPH3 <- EHPrepare_CreateDummies(dfPH2, "PH")
dfPH_Eval3 <- EHPrepare_CreateDummies(dfPH_Eval2, "PH")

```


#### Outliers, Distributions and Correlations 

PH is normally-enough distributed so we will not transform it.  We see some skewed distributions among other factors and may need to correct for that depending on our algorithm choice.  

A number of factors have a wealth of disturbing zeroes (PSC, HydPressure1, etc.). Sometimes zeroes are a stand in for NA.  In a real world case we would use domain knowledge to assess the validity of these zeroes - however, since we don't know, we will assume they are legitimate values.

A fair number of factors are correlated with PH - this bodes well for a predictive model.

```{r}

a <- EHSummarize_StandardPlots(dfPH3, "PH")

```

#### Exploratory Regression Model

We run an ordinary least square regression to gain insight into our data.  We use the Step AIC algorithm which eliminates predictors based on improvements to AIC.

A large number of factors remain in the model.  R-Squared is 41% - we hope to improve this with more powerful predictive algorithms. The model shows heteroskedasticity but it is not overwhelming. The QQ plot suggests a relatively normally distributed target.

There are 3 influential records.  We will remove them.


```{r}

library(MASS)

set.seed(042760)
a <-EHModel_Regression_StandardLM(dfPH3, "PH", xseed="042760", returnLM=TRUE)

```


```{r}

#library(EHData)

dfPH4 <- EHPrepare_RemoveRecordsByRowNumber(dfPH3, c(1094, 690, 2359))
```

#### Machine Learning Models

We now test our data on 11 machine learning algorithms in order to determine the best predictive model.

```{r}

set.seed(042760)

i <- createDataPartition(dfPH4[,1], p = .8, list=FALSE)

dfTrain <- dfPH4[i, ]
dfTest <- dfPH4[-i, ]

dfTrain2 <- dfTrain[,-24]
dfTest2 <- dfTest[,-24]
yTrain <- dfTrain[,24]
yTest <- dfTest[,24]

```


__1). Lasso__

```{r}
library(glmnet)
x <- data.matrix(dfTrain2)
xtest <- data.matrix(dfTest2)

mcv <- cv.glmnet(x, yTrain, alpha = 1)

lambda1 <- mcv$lambda.min

m10 <- glmnet(x, yTrain, alpha = 1, lambda = lambda1, preProc = c("center", "scale"))
m10

lassoPred <- predict(m10, xtest)
postResample(pred =lassoPred, obs = yTest) 

```

__2) Ridge__
```{r}

library(glmnet)
x <- data.matrix(dfTrain2)
xtest <- data.matrix(dfTest2)

mcv <- cv.glmnet(x, yTrain, alpha = 0)

lambda1 <- mcv$lambda.min

m10 <- glmnet(x, yTrain, alpha = 0, lambda = lambda1, preProc = c("center", "scale"))
m10

ridgePred <- predict(m10, xtest)
postResample(pred =ridgePred, obs = yTest) 

```

__3). PLS__

```{r}

ctrl <- trainControl(method = "cv", number = 10)

model2 <- train(dfTrain2, yTrain, method = "pls", metric='Rsquared', tuneLength=50, trControl = ctrl, preProc = c("center", "scale"))
model2

plsPred <- predict(model2, dfTest2)
postResample(pred =plsPred, obs = yTest) 

```

__4.) KNN__
```{r}
set.seed(042760)
knnModel <- train(x = dfTrain2, y = yTrain, method = "knn", preProc = c("center", "scale"), tuneLength = 10)
knnModel

knnPred <- predict(knnModel, newdata = dfTest2)
postResample(pred = knnPred, obs = yTest) 

```

__5). Neural Net__

Decay = .04, size = 4.
```{r}
set.seed(042760)
library(nnet)
nn <- nnet(dfTrain2, yTrain,
                size = 4,
                decay = 0.04,
                linout = TRUE,
                trace = FALSE,
                maxit = 500,
                MaxNWts = 5 * (ncol(dfTrain2) + 1) +5 +1)
nn

nnPred <- predict(nn, newdata = dfTest2)
postResample(pred = nnPred, obs = yTest)


```

__6.) MARS__
```{r}
set.seed(042760)
library(earth)
mars <- earth(dfTrain2, yTrain)
mars

marsPred <- predict(mars, newdata = dfTest2)
postResample(pred = marsPred, obs = yTest)
```

__7.) and 8.) SVM (linear and radial)__

```{r}
set.seed(042760)
library(caret)

svm_Radial <- train(dfTrain2, yTrain,
                   method = "svmRadial",
                   preProc = c("center","scale"),
                   trControl = trainControl(method = "cv"))

svm_Radial

svmPredR <- predict(svm_Radial, newdata = dfTest2)
postResample(pred = svmPredR, obs = yTest)

print('')
print('___________________________________________')
print('')

svm_Linear <- train(dfTrain2, yTrain,
                   method = "svmLinear",
                   preProc = c("center","scale"),
                   tuneLength = 14,
                   trControl = trainControl(method = "cv"))

svm_Linear


svmPredL <- predict(svm_Linear, newdata = dfTest2)
postResample(pred = svmPredL, obs = yTest)


```

__9.) Random Forest__

```{r}
set.seed(042760)
rf <- randomForest(PH ~., data = dfTrain, importance = TRUE, ntree = 1000)
rf

rfPred <- predict(rf, newdata = dfTest2)
postResample(pred =rfPred, obs = yTest) 
```

__10.) GBM__
```{r}
set.seed(042760)
library(gbm)
boosted <- gbm(PH ~., data = dfTrain)
boosted

boostPred <- predict(boosted, newdata = dfTest2)
postResample(pred =boostPred, obs = yTest) 
```

__11.) Cubist__
```{r}

set.seed(042760)
library(Cubist)
cubist <- cubist(dfTrain2, yTrain)
cubist

cubistPred <- predict(cubist,  newdata = dfTest2)
postResample(pred = cubistPred, obs = yTest) 
```
#### Choosing the best model

The table below shows results for the 11 algorithm.  Random Forest is clearly the best choice.

It should be noted that R-Squareds were .05 higher for Random Forest and Support Vector Machines when PH underwent a boxcox transformation.  However, this was not worth the difficulty of interpretability.

```{r}

tab <- matrix(c("OLS Regression", .136, .43, "Lasso", .133, .43, "Ridge", .134, .43, "KNN", .125, .50, "NeuralNet", .120, .54, "MARS", .131, .45, "SVM-Linear", .134, .42,"SVM_Radial",.117,.56, "Random Forest", .099, .72, "Boosted", .132, .46, "Cubist", .133, .49), ncol=3, byrow=TRUE) 
colnames(tab) <- c('Model','RMSE','RSquared')

x <- tab[order(tab[,"RMSE"],decreasing=FALSE),]

tab <- as.table(x)
tab

```

#### Assessing variable importance

First we re-run our regression with scaled factors to get a baseline.

```{r}

dfPH4a <- EHPrepare_ScaleAllButTarget(dfPH4, "PH")
aa <- EHModel_Regression_StandardLM(dfPH4a, "PH", xseed="042760", returnLM = TRUE, tests = FALSE, vif = FALSE)

```

We compare the top ten factors in our regression with those of Random Forest and SVM_Radial (our second best performing model) and look for patterns.

```{r}

print("Top 10 regression coefficients")
  rimp <- as.data.frame(aa$coefficients)
  colnames(rimp)[1] = "Overall"

  rimp <- rimp |>
    dplyr::arrange(desc(abs(Overall)))
  head(rimp, 10)


TopTenVarImp <- function(m)
{
  
  imp <- as.data.frame(varImp(m, scale=FALSE)) |>
    dplyr::arrange(desc(Overall))
  head(imp, 10)
  
}
print("Top 10 Random Forest factors")
  TopTenVarImp(rf)
  
print("Top 10 SVM factors")
  srimp <- varImp(svm_Radial, scale = FALSE) 
  srimp <- srimp$importance |> dplyr::arrange(desc(Overall))
  head(srimp, 10)
```
When we examine the most important predictors, not only in our Random Forest model but in SVM and Regression as well, we find that Mnf.Flow is the most important predictor, followed by Usage.cont and Bowl.Setpoint.  Alch.Rel, Filler.level and Brand (especially Brand C) are also factors. 

When we run an individual tree (see below) we can see that, at least for this tree, Mnf.Flow is the first node. While our Random Forest model aggregates many different trees, we can presume that many of them begin with Mnf.Flow.


```{r}

set.seed(042760)
library(rpart)
library(rpart.plot)

rpart1 <- rpart(PH ~., data = dfPH4)
rpart.plot(rpart1)

#rpart2 <- rpart(PH ~., data = dfPH4, minsplit=700)
#rpart.plot(rpart2)


```

When we re-examine the important predictors, we can see that they all have significant correlations with PH.  While the first three factors are also correlated with each other, the correlations are only about 50%. These are reassuring signs we have a robust model.

```{r}

dfPH_Imp <- dfPH4a |>
  dplyr::select(PH, Mnf.Flow, Bowl.Setpoint, Usage.cont, Alch.Rel, Brand.Code_C )
a <- EHSummarize_StandardPlots(dfPH_Imp, "PH")


```

```{r}
q <- EHExplore_Multicollinearity(dfPH_Imp, threshold=.5, printHighest = TRUE)
```
#### Conclusion

We tested 11 machine learning on beverage data in order to predict PH. OUr final model is tree-based (Random Forest).  The Model R-Squared is 72%, which suggests that our predictions will tend to be in the ballpark but there is still quite a bit of room for small errors up and down.

```{r}

ww <- EHModel_Predict(rf, dfPH_Eval3, "PSC", writeFile = "Predictions.xlsx")


```

